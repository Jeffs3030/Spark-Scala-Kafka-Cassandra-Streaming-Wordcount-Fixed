//start shell
./spark-shell --packages datastax:spark-cassandra-connector:2.0.0-M2-s_2.11
//need to include package for Datastax cassandra connector

//Foreward to code

//I am not a big fan of the wordcount example because it does not take into consideration
//that multiple columns need to be saved into Cassandra 
//This example shows how to use Kafka and how to save into Cassandra using Spark with multiple columns with dataframes

//This code takes a csv formatted kafka stream that is created from a sql source query, 
//the kafka stream values look something like this
//,word1a,word2b,word3b,word4b,word5b,word6d,
//,word1b,word2c,word3c,word4b,word5c,word6e,

//Then we save the Kafka stream into Cassandra as multiple columns
//if you have any questions feel free to ask us at jeffs@jsbusinessintelligence.com
//End foreward to code

// Basic Spark imports

import org.apache.spark.{SparkConf, SparkContext}

import org.apache.spark.SparkContext._


// Spark SQL Cassandra imports

import org.apache.spark.sql

import org.apache.spark.sql.cassandra.CassandraSQLContext

import org.apache.spark.sql.cassandra._

import com.datastax.spark.connector._



// Spark Streaming + Kafka imports

import kafka.serializer.StringDecoder // this has to come before streaming.kafka import

import org.apache.spark.streaming._

import org.apache.spark.streaming.kafka._



// Cassandra Java driver imports

import com.datastax.driver.core.{Session, Cluster, Host, Metadata}

import com.datastax.spark.connector.streaming._

import scala.collection.JavaConversions._



// Date import for processing logic

import java.util.Date


import org.apache.spark.{SparkConf, SparkContext}

import org.apache.spark.sql.{SQLContext, SaveMode}

import org.apache.spark.streaming.{Milliseconds, StreamingContext, Time}

import org.apache.spark.streaming.kafka.KafkaUtils

import kafka.serializer.StringDecoder

import org.apache.spark.rdd.RDD

import java.sql.Timestamp

//configure Spark App Name
val sparkConf = new SparkConf().setAppName("KafkaSparkCassandra")

// get the values we need out of the config file

//configure cassandra
val cassandra_host = sparkConf.get("spark.cassandra.connection.host", "SOMEIP"); //cassandra host

// connect directly to Cassandra from the driver to create the keyspace

//initialize cassanrda
val cluster = Cluster.builder().addContactPoint(cassandra_host).build()
val session = cluster.connect()

//create cassandra tables
session.execute("CREATE TABLE IF NOT EXISTS ic_example.word_countfix (word1 text, word2 text, word3 text, word4 text, word5 text, word6 text, PRIMMARY KEY(word1),word2, word3, word4, word5, word6) ")

//create case class test 1 for word_count example 1
case class Word1case(word1:String, word2:String, word3: String, word4:String, word5: String, word6: String)

//clear out existing data
session.execute("TRUNCATE ic_example.words_countfix")
session.execute("TRUNCATE ic_example.words_count2")

//close out session
session.close()

//stop
sc.stop ()
// Create spark streaming context with 5 second batch interval
val ssc = new StreamingContext(sparkConf, Seconds(5))

// create a timer that we will use to stop the processing after 30 seconds so we can print some results


// Create direct kafka stream with brokers and topics
//configuration for kafka
val zkQuorum ="SOMEIP"
val group = "my-consumer-group"
//need to update the topic name
val topics = "test-topic1"
val numThreads="1"

val kafka_broker = "SOMEIP"

//create topicmap split
val topicMap = topics.split(",").map((_, numThreads.toInt)).toMap

//create stream
val messages = KafkaUtils.createStream(ssc, zkQuorum, group, topicMap).map(_._2)

val messages2=messages

//create data frame to parse,save and store data into Cassandra for additional analytics
//note for data with commas in it you can adjust the delimeter on the split below, eg create ,, parsing
messages2
.foreachRDD { 
(message: RDD[(String)]) => {
val df = message.map {
case (v)   => v.split(',')
}.map(payload => {
Word1case(payload(1), payload(2), payload(3), payload(4), payload(5), payload(6))
//need to update the fields used here
}).toDF("word1", "word2", "word3", "word4", "word5", "word6")

df
.write
.format("org.apache.spark.sql.cassandra")
.mode(SaveMode.Append)
//need to update the TABLE SAVED HERe
//need to create the table to save the data
.options(Map("keyspace" -> "ic_example", "table" -> "word_countfix"))
.save()		
}
}

//start stream and save to cassandra process
ssc.start()